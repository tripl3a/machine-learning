---
title: "Machine Learning 1 - Workshop 8 - Non-Linear Regression Models"
author: "Arndt Allhorn"
date: "May 29, 2018"
output: html_notebook
---

# Non-linear modelling: Wage data set

Work through Lab 7.8 in James et al. starting on page 287 until the end of the section on loess p. 294. The
detail of local regression (loess) come next week, but it doesn’t hurt to work through the few commands
here.  
Ignore the part on pages 291 and 292 between “Next we consider the task of predicting whether an
individual earns more than $250,000 per year” and ‘rug plot”. This part deals with classification via a
logistic regression. A subject you will learn this week in Regression.

```{r}
library (ISLR)
attach (Wage)
```

## 7.8.1 Polynomial Regression and Step Functions

We now examine how Figure 7.1 was produced. We first fit the model using the following command:

```{r}
fit=lm(wage~poly(age,4), data=Wage)
coef(summary(fit))
```

This syntax fits a linear model, using the `lm()` function, in order to predict `wage` using a fourth-degree polynomial in `age: poly(age,4)`. The `poly()` command allows us to avoid having to write out a long formula with powers of age. The function returns a matrix whose columns are a basis of _orthogonal polynomials_, which essentially means that each column is a linear combination of the variables `age`, `age^2`, `age^3` and `age^4`.  

However, we can also use `poly()` to obtain `age`, `age^2`, `age^3` and `age^4`
directly, if we prefer. We can do this by using the `raw=TRUE` argument to
the `poly()` function. Later we see that this does not affect the model in a
meaningful way—though the choice of basis clearly affects the coefficient
estimates, it does not affect the fitted values obtained.

```{r}
fit2=lm(wage~poly(age,4,raw=T), data=Wage)
coef(summary(fit2))
```

We now create a grid of values for `age` at which we want predictions, and
then call the generic `predict()` function, specifying that we want standard
errors as well.

```{r}
agelims = range (age )
age.grid=seq (from= agelims [1], to= agelims [2])
preds = predict (fit , newdata =list (age =age.grid),se=TRUE)
se.bands = cbind ( preds$fit +2* preds$se.fit ,preds$fit -2* preds$se.fit )
```

Finally, we plot the data and add the fit from the degree-4 polynomial.

```{r}
par ( mfrow =c(1 ,2) ,mar =c(4.5 ,4.5 ,1 ,1) ,oma =c(0 ,0 ,4 ,0) )
plot(age ,wage ,xlim =agelims ,cex =.5, col ="darkgrey")
title ("Degree -4 Polynomial", outer =T)
lines (age.grid , preds$fit ,lwd =2, col ="blue")
matlines (age.grid ,se.bands ,lwd =1, col ="blue",lty =3)
```
Here the `mar` and `oma` arguments to `par()` allow us to control the margins
of the plot, and the `title()` function creates a figure title that spans both
title() subplots.  

We mentioned earlier that whether or not an orthogonal set of basis functions is produced in the `poly()` function will not affect the model obtained in a meaningful way. 
What do we mean by this? The fitted values obtained in either case are identical:

```{r}
preds2 = predict (fit2 , newdata =list( age =age.grid),se= TRUE)
max ( abs (preds$fit - preds2$fit ))
```

In performing a polynomial regression we must decide on the degree of
the polynomial to use. One way to do this is by using hypothesis tests. We
now fit models ranging from linear to a degree-5 polynomial and seek to
determine the simplest model which is sufficient to explain the relationship
between `wage` and `age`.

We use the `anova()` function, which performs an
analysis of variance (ANOVA, using an F-test) in order to test the null
hypothesis that a model M1 is sufficient to explain the data against the variance
alternative hypothesis that a more complex model M2 is required. In order
to use the `anova()` function, M1 and M2 must be _nested_ models: the
predictors in M1 must be a subset of the predictors in M2. In this case,
we fit five different models and sequentially compare the simpler model to
the more complex model.

```{r}
fit.1= lm( wage~age , data=Wage )
fit.2= lm( wage~poly (age ,2) ,data =Wage)
fit.3= lm( wage~poly (age ,3) ,data =Wage)
fit.4= lm( wage~poly (age ,4) ,data =Wage)
fit.5= lm( wage~poly (age ,5) ,data =Wage)
anova (fit.1, fit.2, fit.3, fit.4, fit.5)
```
The p-value comparing the linear `Model 1` to the quadratic `Model 2` is
essentially zero (<10−15), indicating that a linear fit is not sufficient. Similarly the p-value comparing the quadratic `Model 2` to the cubic `Model 3` is very low (0.0017), so the quadratic fit is also insufficient. The p-value
comparing the cubic and degree-4 polynomials, `Model 3` and `Model 4`, is approximately 5 % while the degree-5 polynomial `Model 5` seems unnecessary because its p-value is 0.37. Hence, either a cubic or a quartic polynomial
appear to provide a reasonable fit to the data, but lower- or higher-order
models are not justified.

In this case, instead of using the `anova()` function, we could have obtained
these p-values more succinctly by exploiting the fact that `poly()` creates
orthogonal polynomials.

```{r}
coef( summary (fit.5) )
```
Notice that the p-values are the same, and in fact the square of the
t-statistics are equal to the F-statistics from the `anova()` function; for
example:

```{r}
( -11.983) ^2
```

However, the ANOVA method works whether or not we used orthogonal
polynomials; it also works when we have other terms in the model as well.
For example, we can use `anova()` to compare these three models:

```{r}
fit.1= lm( wage~education +age ,data= Wage)
fit.2= lm( wage~education + poly(age ,2) ,data= Wage)
fit.3= lm( wage~education + poly(age ,3) ,data= Wage)
anova (fit.1, fit.2, fit.3)
```

As an alternative to using hypothesis tests and ANOVA, we could choose
the polynomial degree using cross-validation, as discussed in Chapter 5.

**skip some part of the pages 291 and 292**

In order to fit a step function, as discussed in Section 7.2, we use the
cut() function.

```{r}
table (cut (age ,4) )
fit =lm(wage~cut (age ,4) ,data =Wage)
coef( summary (fit ))
```

Here `cut()` automatically picked the cutpoints at 33.5, 49, and 64.5 years
of age. We could also have specified our own cutpoints directly using the
`breaks` option. The function `cut()` returns an ordered categorical variable;
the `lm()` function then creates a set of dummy variables for use in the regression. The `age<33.5` category is left out, so the intercept coefficient of
$94,160 can be interpreted as the average salary for those under 33.5 years
of age, and the other coefficients can be interpreted as the average additional salary for those in the other age groups. We can produce predictions
and plots just as we did in the case of the polynomial fit.

## 7.8.2 Splines

In order to fit regression splines in R, we use the `splines` library.
In Section 7.4, we saw that regression splines can be fit by constructing an appropriate matrix of basis functions. The `bs()` function generates the entire matrix of basis functions for splines with the specified set of knots. By default, cubic splines are produced. Fitting `wage` to `age` using a regression spline is simple:

```{r}
# copied code from above
par ( mfrow =c(1 ,2) ,mar =c(4.5 ,4.5 ,1 ,1) ,oma =c(0 ,0 ,4 ,0) )
plot(age ,wage ,xlim =agelims ,cex =.5, col ="darkgrey")
title ("Degree -4 Polynomial", outer =T)
lines (age.grid , preds$fit ,lwd =2, col ="blue")
matlines (age.grid ,se.bands ,lwd =1, col ="blue",lty =3)
# new code
library ( splines )
fit =lm(wage~bs(age ,knots =c(25 ,40 ,60) ),data=Wage )
pred= predict (fit , newdata =list(age =age.grid),se=T)
plot(age ,wage ,col ="gray")
lines (age.grid , pred$fit ,lwd =2)
lines (age.grid , pred$fit +2* pred$se ,lty ="dashed")
lines (age.grid , pred$fit -2* pred$se ,lty ="dashed")
```

Here we have prespecified knots at ages 25, 40, and 60. This produces a
spline with six basis functions. (Recall that a cubic spline with three knots
has seven degrees of freedom; these degrees of freedom are used up by an
intercept, plus six basis functions.)
We could also use the `df` option to produce a spline with knots at uniform quantiles of the data.

```{r}
print(dim (bs(age , knots =c (25 ,40 ,60) )))
print(dim (bs(age ,df =6) ))
print(attr(bs(age ,df =6) ,"knots"))
```

In this case R chooses knots at ages 33.8,42.0, and 51.0, which correspond
to the 25th, 50th, and 75th percentiles of `age`. The function `bs()` also has
a `degree` argument, so we can fit splines of any degree, rather than the
default degree of 3 (which yields a cubic spline).  

In order to instead fit a natural spline, we use the `ns()` function. Here we fit a natural spline with four degrees of freedom.

```{r}
fit2=lm(wage~ns(age ,df =4) ,data =Wage)
pred2 = predict (fit2 , newdata = list(age = age.grid ),se=T)
plot(age ,wage ,col =" gray ")
lines (age.grid , pred2$fit ,col =" red ", lwd =2)
```

As with the `bs()` function, we could instead specify the knots directly using
the `knots` option.

In order to fit a smoothing spline, we use the `smooth.spline()` function.  
Figure 7.8 was produced with the following code:

```{r}
plot(age ,wage ,xlim =agelims ,cex =.5, col =" darkgrey ")
title (" Smoothing Spline ")
fit = smooth.spline (age ,wage ,df =16)
fit2= smooth.spline (age ,wage ,cv=TRUE)
print(fit2$df)
lines (fit ,col =" red ",lwd =2)
lines (fit2 ,col =" blue", lwd =2)
legend ("topright", legend =c("16 DF" ,"6.8 DF") ,
col =c("red","blue") ,lty =1, lwd =2, cex =.8)
```

Notice that in the first call to `smooth.spline()`, we specified `df=16`. The
function then determines which value of λ leads to 16 degrees of freedom. In
the second call to `smooth.spline()`, we select the smoothness level by crossvalidation; this results in a value of λ that yields 6.8 degrees of freedom.

In order to perform local regression, we use the `loess()` function.

```{r}
plot(age ,wage ,xlim =agelims ,cex =.5, col =" darkgrey ")
title (" Local Regression ")
fit = loess (wage~age ,span =.2, data= Wage)
fit2= loess (wage~age ,span =.5, data= Wage)
lines (age.grid , predict (fit , data.frame (age = age.grid )),
col =" red ",lwd =2)
lines (age.grid , predict (fit2 ,data.frame ( age =age.grid)),
col =" blue",lwd =2)
legend ("topright", legend =c("Span=0.2" ,"Span=0.5") ,
col =c(" red "," blue ") ,lty =1, lwd =2, cex =.8)
```

Here we have performed local linear regression using spans of 0.2 and 0.5:
that is, each neighborhood consists of 20 % or 50 % of the observations. The
larger the span, the smoother the fit. The `locfit` library can also be used
for fitting local regression models in R.
